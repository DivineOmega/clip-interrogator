{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DivineOmega/clip-interrogator/blob/main/clip_interrogator.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ytxkysgmrJEi"
      },
      "source": [
        "# CLIP Interrogator by [@pharmapsychotic](https://twitter.com/pharmapsychotic) \n",
        "\n",
        "<br>\n",
        "\n",
        "What do the different OpenAI CLIP models see in an image? What might be a good text prompt to create similar images using CLIP guided diffusion or another text to image model? The CLIP Interrogator is here to get you answers!\n",
        "\n",
        "<br>\n",
        "\n",
        "If this notebook is helpful to you please consider buying me a coffee via [ko-fi](https://ko-fi.com/pharmapsychotic) or following me on [twitter](https://twitter.com/pharmapsychotic) for more cool Ai stuff. ðŸ™‚\n",
        "\n",
        "And if you're looking for more Ai art tools check out my [Ai generative art tools list](https://pharmapsychotic.com/tools.html).\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "YQk0eemUrSC7",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b4abe440-afab-4a27-8be4-3f35b158847c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "GPU 0: Tesla P100-PCIE-16GB (UUID: GPU-27fdc045-488a-714f-6421-8ab5ecb2e293)\n"
          ]
        }
      ],
      "source": [
        "#@title Check GPU\n",
        "!nvidia-smi -L"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {
        "id": "30xPxDSDrJEl",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4bfada77-b78c-4a0c-be8d-e2e0085ea3fd"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (4.64.0)\n",
            "Requirement already satisfied: transformers==4.15.0 in /usr/local/lib/python3.7/dist-packages (4.15.0)\n",
            "Requirement already satisfied: timm==0.4.12 in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: fairscale==0.4.4 in /usr/local/lib/python3.7/dist-packages (0.4.4)\n",
            "Requirement already satisfied: tokenizers<0.11,>=0.10.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (0.10.3)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (2.23.0)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (3.8.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (1.21.6)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.1.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (0.9.1)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (4.12.0)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (0.0.53)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (21.3)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.7/dist-packages (from transformers==4.15.0) (6.0)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm==0.4.12) (0.13.1+cu113)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.7/dist-packages (from huggingface-hub<1.0,>=0.1.0->transformers==4.15.0) (4.1.1)\n",
            "Requirement already satisfied: pyparsing!=3.0.5,>=2.0.2 in /usr/local/lib/python3.7/dist-packages (from packaging>=20.0->transformers==4.15.0) (3.0.9)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy) (0.2.5)\n",
            "Requirement already satisfied: zipp>=0.5 in /usr/local/lib/python3.7/dist-packages (from importlib-metadata->transformers==4.15.0) (3.8.1)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (2022.6.15)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (2.10)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->transformers==4.15.0) (1.24.3)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.15.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.15.0) (1.1.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.7/dist-packages (from sacremoses->transformers==4.15.0) (1.15.0)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm==0.4.12) (7.1.2)\n",
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting git+https://github.com/openai/CLIP.git\n",
            "  Cloning https://github.com/openai/CLIP.git to /tmp/pip-req-build-b43ckv_o\n",
            "  Running command git clone -q https://github.com/openai/CLIP.git /tmp/pip-req-build-b43ckv_o\n",
            "Requirement already satisfied: ftfy in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (6.1.1)\n",
            "Requirement already satisfied: regex in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (2022.6.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (4.64.0)\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (1.12.1+cu113)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from clip==1.0) (0.13.1+cu113)\n",
            "Requirement already satisfied: wcwidth>=0.2.5 in /usr/local/lib/python3.7/dist-packages (from ftfy->clip==1.0) (0.2.5)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch->clip==1.0) (4.1.1)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (7.1.2)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (2.23.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->clip==1.0) (1.21.6)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (1.24.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2022.6.15)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (2.10)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.7/dist-packages (from requests->torchvision->clip==1.0) (3.0.4)\n",
            "fatal: destination path 'clip-interrogator' already exists and is not an empty directory.\n"
          ]
        }
      ],
      "source": [
        "#@title Setup\n",
        "!pip3 install ftfy regex tqdm transformers==4.15.0 timm==0.4.12 fairscale==0.4.4\n",
        "!pip3 install git+https://github.com/openai/CLIP.git\n",
        "!git clone https://github.com/pharmapsychotic/clip-interrogator.git\n",
        "\n",
        "import clip\n",
        "import gc\n",
        "import numpy as np\n",
        "import os\n",
        "import pandas as pd\n",
        "import requests\n",
        "import torch\n",
        "import torchvision.transforms as T\n",
        "import torchvision.transforms.functional as TF\n",
        "\n",
        "from IPython.display import display\n",
        "from PIL import Image\n",
        "from torch import nn\n",
        "from torch.nn import functional as F\n",
        "from torchvision import transforms\n",
        "from torchvision.transforms.functional import InterpolationMode\n",
        "\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
        "\n",
        "def load_list(filename):\n",
        "    with open(filename, 'r', encoding='utf-8', errors='replace') as f:\n",
        "        items = [line.strip() for line in f.readlines()]\n",
        "    return items\n",
        "\n",
        "def rank(model, image_features, text_array, top_count=1):\n",
        "    top_count = min(top_count, len(text_array))\n",
        "    text_tokens = clip.tokenize([text for text in text_array]).cuda()\n",
        "    with torch.no_grad():\n",
        "        text_features = model.encode_text(text_tokens).float()\n",
        "    text_features /= text_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    similarity = torch.zeros((1, len(text_array))).to(device)\n",
        "    for i in range(image_features.shape[0]):\n",
        "        similarity += (100.0 * image_features[i].unsqueeze(0) @ text_features.T).softmax(dim=-1)\n",
        "    similarity /= image_features.shape[0]\n",
        "\n",
        "    top_probs, top_labels = similarity.cpu().topk(top_count, dim=-1)  \n",
        "    return [(text_array[top_labels[0][i].numpy()], (top_probs[0][i].numpy()*100)) for i in range(top_count)]\n",
        "\n",
        "def interrogate(image, models):\n",
        "\n",
        "    if len(models) != 1:\n",
        "        print(\"You must select exactly one model.\")\n",
        "        return\n",
        "\n",
        "    table = []\n",
        "    #bests = [[('',0)]]*5\n",
        "\n",
        "    model_name = models[0]\n",
        "\n",
        "    print(f\"Interrogating with {model_name}...\")\n",
        "    model, preprocess = clip.load(model_name)\n",
        "    model.cuda().eval()\n",
        "\n",
        "    images = preprocess(image).unsqueeze(0).cuda()\n",
        "    with torch.no_grad():\n",
        "        image_features = model.encode_image(images).float()\n",
        "    image_features /= image_features.norm(dim=-1, keepdim=True)\n",
        "\n",
        "    ranks = [\n",
        "        rank(model, image_features, [f\"{x} photo of a person\" for x in focal_lengths]),\n",
        "        rank(model, image_features, [f\"photo of a {x} person\" for x in styles]),\n",
        "        rank(model, image_features, [f\"photo of a {x} person\" for x in ages]),\n",
        "        rank(model, image_features, [f\"photo of a {x}\" for x in genders]),\n",
        "        rank(model, image_features, [f\"photo of a person with {x} length hair\" for x in hair_lengths]),\n",
        "        rank(model, image_features, [f\"photo of a person with {x} coloured hair\" for x in hair_colours]),\n",
        "        rank(model, image_features, [f\"photo of a person with a {x} shaped_face\" for x in face_shapes]),\n",
        "        rank(model, image_features, [f\"photo of a person with {x} eyes\" for x in eye_colours]),\n",
        "        rank(model, image_features, [f\"photo of a person wearing a {x}\" for x in top_garments]),\n",
        "        rank(model, image_features, [f\"photo of a person wearing a {x}\" for x in bottom_garments]),\n",
        "        rank(model, image_features, [f\"photo of a person wearing a {x}\" for x in outer_garments]),\n",
        "        rank(model, image_features, [f\"photo of a person with a {x} background\" for x in backgrounds]),\n",
        "        rank(model, image_features, [f\"photo of a person wearing {x}\" for x in glasses]),\n",
        "        rank(model, image_features, [f\"photo of a person {x}\" for x in poses]),\n",
        "        rank(model, image_features, [f\"{x} photo of a person\" for x in photo_styles]),\n",
        "        rank(model, image_features, [f\"photo of a {x} person\" for x in nationalities])\n",
        "        # rank(model, image_features, flavors, top_count=3)\n",
        "    ]\n",
        "\n",
        "    #print (ranks[5])\n",
        "\n",
        "    #for i in range(len(ranks)):\n",
        "    #    confidence_sum = 0\n",
        "    #    for ci in range(len(ranks[i])):\n",
        "    #        confidence_sum += ranks[i][ci][1]\n",
        "    #    if confidence_sum > sum(bests[i][t][1] for t in range(len(bests[i]))):\n",
        "    #        bests[i] = ranks[i]\n",
        "\n",
        "    #row = [model_name]\n",
        "    #for r in ranks:\n",
        "    #    row.append(', '.join([f\"{x[0]} ({x[1]:0.1f}%)\" for x in r]))\n",
        "\n",
        "    #table.append(row)\n",
        "\n",
        "    gc.collect()\n",
        "\n",
        "    #display(pd.DataFrame(table, columns=[\"Model\", \"Focal Length\", \"Style\", \"Age\", \"Gender\", \"Hair Length\", \"Hair Colour\", \"Face Shape\", \"Eye Colour\", \"Top Garment\", \"Bottom Garment\", \"Outer Garment\"]))\n",
        "\n",
        "    focal_length = find_original_attribute(ranks[0][0][0], focal_lengths)\n",
        "    style = find_original_attribute(ranks[1][0][0], styles)\n",
        "    age = find_original_attribute(ranks[2][0][0], ages)\n",
        "    gender = find_original_attribute(ranks[3][0][0], genders)\n",
        "    hair_length = find_original_attribute(ranks[4][0][0], hair_lengths)\n",
        "    hair_colour = find_original_attribute(ranks[5][0][0], hair_colours)\n",
        "    face_shape = find_original_attribute(ranks[6][0][0], face_shapes)\n",
        "    eye_colour = find_original_attribute(ranks[7][0][0], eye_colours)\n",
        "    top_garment = find_original_attribute(ranks[8][0][0], top_garments)\n",
        "    bottom_garment = find_original_attribute(ranks[9][0][0], bottom_garments)\n",
        "    outer_garment = find_original_attribute(ranks[10][0][0], outer_garments)\n",
        "    background = find_original_attribute(ranks[11][0][0], backgrounds)\n",
        "    glasses_singular = find_original_attribute(ranks[12][0][0], glasses)\n",
        "    pose = find_original_attribute(ranks[13][0][0], poses)\n",
        "    photo_style = find_original_attribute(ranks[14][0][0], photo_styles)\n",
        "    nationality = find_original_attribute(ranks[15][0][0], nationalities)\n",
        "\n",
        "    garments = []\n",
        "    if (top_garment != 'no top garment'):\n",
        "        garment_colour_rank = rank(model, image_features, [f\"photo of a person wearing a {x} {top_garment}\" for x in garment_colours])\n",
        "        garment_colour = find_original_attribute(garment_colour_rank[0][0], garment_colours)\n",
        "        garments.append('a ' + garment_colour + ' ' + top_garment)\n",
        "    if (bottom_garment != 'no bottom garment'):\n",
        "        garment_colour_rank = rank(model, image_features, [f\"photo of a person wearing a {x} {bottom_garment}\" for x in garment_colours])\n",
        "        garment_colour = find_original_attribute(garment_colour_rank[0][0], garment_colours)\n",
        "        garments.append('a ' + garment_colour + ' ' +bottom_garment)\n",
        "    if (outer_garment != 'no outer garment'):\n",
        "        garment_colour_rank = rank(model, image_features, [f\"photo of a person wearing a {x} {outer_garment}\" for x in garment_colours])\n",
        "        garment_colour = find_original_attribute(garment_colour_rank[0][0], garment_colours)\n",
        "        garments.append('a ' + garment_colour + ' ' + outer_garment)\n",
        "    if (glasses_singular != 'no glasses'):\n",
        "        garment_colour_rank = rank(model, image_features, [f\"photo of a person wearing {x} {glasses_singular}\" for x in garment_colours])\n",
        "        garment_colour = find_original_attribute(garment_colour_rank[0][0], garment_colours)\n",
        "        garments.append(garment_colour + ' ' + glasses_singular)\n",
        "\n",
        "    wearing = ''\n",
        "    if (len(garments) > 0):\n",
        "        wearing += 'wearing ';\n",
        "        for i, garment in enumerate(garments, start=0):\n",
        "            wearing += garment\n",
        "            if (i < len(garments)-1):\n",
        "              wearing += ' and '\n",
        "\n",
        "    print(f\"\\n\\n{focal_length} {photo_style} photo of a {style} {age} {nationality} {gender} {pose} with {hair_length} {hair_colour} hair, a {face_shape} face and {eye_colour} eyes {wearing}. {background} background. dof. bokeh. extremely detailed. Nikon D850.\")\n",
        "\n",
        "def find_original_attribute(best, original_attributes):\n",
        "  for original_attribute in original_attributes:\n",
        "        if (' '+original_attribute+' ' in ' '+best+' '):\n",
        "            return original_attribute\n",
        "            \n",
        "\n",
        "data_path = \"../clip-interrogator/data/\"\n",
        "\n",
        "focal_lengths = ['18mm', '24mm', '35mm', '55mm', '85mm', '105mm', '135mm', '200mm'];\n",
        "styles = ['tattoed', 'professional', 'beautiful', 'attractive','sexy naked', 'sexy', 'naked', 'ugly', 'average looking', 'stylish', 'glamourous', 'elegant', 'cyberpunk', 'steampunk', 'handsome', 'goth', 'emo', 'alternative', 'punk', 'pretty', 'cute']\n",
        "ages = ['18 year old', '20 year old', '30 year old', '40 year old', '50 year old', '60 year old', '70 year old', '80 year old', '90 year old', '100 year old']\n",
        "genders = ['woman', 'man']\n",
        "hair_lengths = ['short bob', 'medium bob', 'long bob', 'long pony tail', 'pony tail','long straight', 'straight shoulder length', 'straight waist length', 'long wavey', 'wavey shoulder length', 'wavey waist length', 'waist length', 'shoulder length', 'pixie cut', 'very long', 'very short', 'long', 'medium', 'short']\n",
        "hair_colours = ['dirty blond', 'bleach blond', 'blond', 'light brown', 'dark brown', 'brown', 'black', 'light red', 'bright red', 'dark red', 'red', 'ginger', 'very white', 'white', 'grey', 'gray', 'green', 'blue', 'cyan', 'orange', 'purple', 'pink', 'gold']\n",
        "face_shapes = ['diamond', 'heart', 'oval', 'rectangular', 'round', 'square']\n",
        "eye_colours = ['dark brown', 'chocolate brown', 'light brown', 'light blue', 'dark blue', 'light green', 'dark green', 'blue', 'brown', 'grey', 'gray', 'green']\n",
        "top_garments = ['no top garment', 'Anna (Frozen) dress', 'Elsa (Frozen) dress', 'pink and blue patterned top' 'sleveless button down top', 'sleveless top', 'summer dress', 'military style top', 'headscarf', 'hijab', 'hooded garment', 'long dress', 'short dress', 'tank top', 'tube top', 'crop top', 'halter top', 'off-the-shoulder top', 'one-shoulder top', 't-shirt', 'blouse', 'button-down shirt', 'polo shirt', 'bikini', 'bra', 'shirt and tie', 'shirt', 'catsuit', 'dress', 'rags', 'princess dress', 'wedding dress']\n",
        "bottom_garments = ['no bottom garment', 'fishnet tights', 'denim jeans', 'short denim shorts', 'short ripped denim shorts', 'short denim dress', 'denim shorts', 'denim dress', 'skirt', 'short skirt', 'long skirt', 'skort', 'shorts', 'short shorts', 'long shorts', 'jeans', 'capris', 'pants', 'trousers', 'tutu']\n",
        "outer_garments = ['no outer garment', 'fancy dress', 'super hero costume', 'wonder woman costume', 'superman constume', 'american flag', 'united kingdom flag', 'flag', 'coat', 'cardigan', 'jacket', 'hoodie', 'robe', 'suit jacket', 'sweater']\n",
        "glasses = ['no glasses', 'cat eye contact lenses', 'contact lenses', 'round glasses', 'square glasses', 'rectangular glasses', 'cat eye glasses', 'sunglasses', 'glasses']\n",
        "garment_colours = ['red with small white flowers', 'invisible', 'partially visible', 'transparent', 'semi-transparent', 'see through', 'black latex', 'latex', 'yellow and black tiger print', 'light pink', 'light blue', 'dark blue', 'black and white horizontally striped', 'silver and gold metallic', 'polka dot', 'dotted', 'horizontally striped', 'vertically striped', 'stripped', 'cosplay', 'black and white', 'leopard print', 'tiger print', 'lion print', 'animal print', 'traditional', 'silver metallic', 'red metallic', 'green metallic', 'blue metallic', 'metallic', 'silver sequinned', 'sequinned', 'intricately patterned', 'frilly white', 'frilly black', 'frilly', 'geometric', 'patterned', 'flowery', 'floral', 'blue leather', 'black leather', 'leather', 'red', 'green', 'blue', 'white', 'black', 'orange', 'yellow', 'purple', 'pink', 'grey', 'gray', 'brown', 'gold', 'cyan']\n",
        "backgrounds = ['snowy', 'mountain top', 'clothes shop', 'leaves on the ground', 'night time street light', 'roof', 'train station', 'British street', 'American street', 'street', 'inside of a car', 'crowd of people', 'sunset', 'building alongside a river', 'building', 'river', 'water', 'desert', 'grassy field', 'field of flowers', 'field', 'roof top', 'cyberpunk', 'steampunk', 'science fiction', 'city', 'industrial', 'white wall', 'brick wall', 'white kitchen', 'park', 'office', 'bedroom', 'kitchen', 'white', 'black', 'gray', 'grey', 'red', 'yellow', 'light blue', 'blue', 'cyan', 'orange', 'colourful', 'light', 'dark']\n",
        "poses = ['holding a flower', 'posing with hands on hips', 'posing with one hand pointing at the camera', 'posing with hands on lap', 'hands on lap', 'playing a violin', 'playing a musical instrument', 'smoking a cigar', 'sitting at a desk writing on a sheet of paper', 'writing on a sheet of paper', 'sitting at a desk', 'eyes closed and arms down and out to the side', 'eyes closed and arms out to the side', 'yoga pose', 'arms crossed', 'smiling and looking directly at the camera', 'face palming', 'posing with a happy smile', 'standing with a happy smile', 'sitting with arms on legs with a happy smile', 'sitting with a happy smile', 'posing with both hands in hair', 'posing with one hand in hair', 'pulling a funny face and holding fingers up like horns above head', 'holding fingers up like horns above head', 'pulling a funny face', 'looking at the camera with mouth open', 'posing with eyes closed', 'standing posing with their mobile phone', 'posing with their mobile phone', 'smoking', 'posing with tongue sticking out', 'posing with chest forward', 'posing seductively', 'posing with finger on lips', 'posing with finger on face', 'resting', 'relaxing against a wall', 'standing against a wall', 'relaxing', 'sitting cross legged', 'standing', 'sitting', 'lying', 'stretching', 'exercising', 'kissing', 'walking', 'running', 'riding a bike', 'riding a house', 'driving a car', 'punching', 'kicking', 'fighting', 'posing', 'pouting']\n",
        "photo_styles = ['selfie', 'side portrait', 'portrait', 'extreme close up', 'close up', 'full body', 'wide angle', 'distant', 'zoomed in', 'zoomed out']\n",
        "nationalities = ['Afghan', 'Albanian', 'Algerian', 'American', 'Andorran', 'Angolan', 'Antiguans', 'Argentinean', 'Armenian', 'Australian', 'Austrian', 'Azerbaijani', 'Bahamian', 'Bahraini', 'Bangladeshi', 'Barbadian', 'Barbudans', 'Batswana', 'Belarusian', 'Belgian', 'Belizean', 'Beninese', 'Bhutanese', 'Bolivian', 'Bosnian', 'Brazilian', 'British', 'Bruneian', 'Bulgarian', 'Burkinabe', 'Burmese', 'Burundian', 'Cambodian', 'Cameroonian', 'Canadian', 'Cape Verdean', 'Central African', 'Chadian', 'Chilean', 'Chinese', 'Colombian', 'Comoran',  'Congolese', 'Costa Rican', 'Croatian', 'Cuban', 'Cypriot', 'Czech', 'Danish', 'Djibouti', 'Dominican', 'Dutch', 'Dutchman', 'Dutchwoman', 'East Timorese', 'Ecuadorean', 'Egyptian', 'Emirian', 'Equatorial Guinean', 'Eritrean', 'Estonian', 'Ethiopian', 'Fijian', 'Filipino', 'Finnish', 'French', 'Gabonese', 'Gambian', 'Georgian', 'German', 'Ghanaian', 'Greek', 'Grenadian', 'Guatemalan', 'Guinea-Bissauan', 'Guinean', 'Guyanese', 'Haitian', 'Herzegovinian', 'Honduran', 'Hungarian', 'I-Kiribati', 'Icelander', 'Indian', 'Indonesian', 'Iranian', 'Iraqi', 'Irish', 'Israeli', 'Italian', 'Ivorian', 'Jamaican', 'Japanese', 'Jordanian', 'Kazakhstani', 'Kenyan', 'Kittian and Nevisian', 'Kuwaiti', 'Kyrgyz', 'Laotian', 'Latvian', 'Lebanese', 'Liberian', 'Libyan', 'Liechtensteiner', 'Lithuanian', 'Luxembourger', 'Macedonian', 'Malagasy', 'Malawian', 'Malaysian', 'Maldivan', 'Malian', 'Maltese', 'Marshallese', 'Mauritanian', 'Mauritian', 'Mexican', 'Micronesian', 'Moldovan', 'Monacan', 'Mongolian', 'Moroccan', 'Mosotho', 'Motswana', 'Mozambican', 'Namibian', 'Nauruan', 'Nepalese', 'Netherlander', 'New Zealander', 'Ni-Vanuatu', 'Nicaraguan', 'Nigerian', 'Nigerien', 'North Korean', 'Northern Irish', 'Norwegian', 'Omani', 'Pakistani', 'Palauan', 'Panamanian', 'Papua New Guinean', 'Paraguayan', 'Peruvian', 'Polish', 'Portuguese', 'Qatari', 'Romanian', 'Russian', 'Rwandan', 'Saint Lucian', 'Salvadoran', 'Samoan', 'San Marinese', 'Sao Tomean', 'Saudi', 'Scottish', 'Senegalese', 'Serbian', 'Seychellois', 'Sierra Leonean', 'Singaporean', 'Slovakian', 'Slovenian', 'Solomon Islander', 'Somali', 'South African', 'South Korean', 'Spanish', 'Sri Lankan', 'Sudanese', 'Surinamer', 'Swazi', 'Swedish', 'Swiss', 'Syrian', 'Taiwanese', 'Tajik', 'Tanzanian', 'Thai', 'Togolese', 'Tongan', 'Trinidadian or Tobagonian', 'Tunisian', 'Turkish', 'Tuvaluan', 'Ugandan', 'Ukrainian', 'Uruguayan', 'Uzbekistani', 'Venezuelan', 'Vietnamese', 'Welsh', 'Yemenite', 'Zambian', 'Zimbabwean']"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "cellView": "form",
        "id": "rbDEMDGJrJEo"
      },
      "outputs": [],
      "source": [
        "#@title Interrogate!\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**Image:**\n",
        "\n",
        "image_path_or_url = \"\" #@param {type:\"string\"}\n",
        "\n",
        "#@markdown \n",
        "\n",
        "#@markdown #####**CLIP models:**\n",
        "\n",
        "#@markdown For [StableDiffusion](https://stability.ai/blog/stable-diffusion-announcement) you can just use ViTL14<br>\n",
        "#@markdown For [DiscoDiffusion](https://colab.research.google.com/github/alembics/disco-diffusion/blob/main/Disco_Diffusion.ipynb) and \n",
        "#@markdown [JAX](https://colab.research.google.com/github/huemin-art/jax-guided-diffusion/blob/v2.7/Huemin_Jax_Diffusion_2_7.ipynb) enable all the same models here as you intend to use when generating your images\n",
        "\n",
        "ViTB32 = False #@param{type:\"boolean\"}\n",
        "ViTB16 = False #@param{type:\"boolean\"}\n",
        "ViTL14 = True #@param{type:\"boolean\"}\n",
        "ViTL14_336px = False #@param{type:\"boolean\"}\n",
        "RN101 = False #@param{type:\"boolean\"}\n",
        "RN50 = False #@param{type:\"boolean\"}\n",
        "RN50x4 = False #@param{type:\"boolean\"}\n",
        "RN50x16 = False #@param{type:\"boolean\"}\n",
        "RN50x64 = False #@param{type:\"boolean\"}\n",
        "\n",
        "models = []\n",
        "if ViTB32: models.append('ViT-B/32')\n",
        "if ViTB16: models.append('ViT-B/16')\n",
        "if ViTL14: models.append('ViT-L/14')\n",
        "if ViTL14_336px: models.append('ViT-L/14@336px')\n",
        "if RN101: models.append('RN101')\n",
        "if RN50: models.append('RN50')\n",
        "if RN50x4: models.append('RN50x4')\n",
        "if RN50x16: models.append('RN50x16')\n",
        "if RN50x64: models.append('RN50x64')\n",
        "\n",
        "if str(image_path_or_url).startswith('http://') or str(image_path_or_url).startswith('https://'):\n",
        "    image = Image.open(requests.get(image_path_or_url, stream=True).raw).convert('RGB')\n",
        "else:\n",
        "    image = Image.open(image_path_or_url).convert('RGB')\n",
        "\n",
        "thumb = image.copy()\n",
        "thumb.thumbnail([256, 256])\n",
        "display(thumb)\n",
        "\n",
        "interrogate(image, models=models)\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [],
      "name": "clip-interrogator.ipynb",
      "provenance": [],
      "include_colab_link": true
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.5 ('base')",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.5"
    },
    "orig_nbformat": 4,
    "vscode": {
      "interpreter": {
        "hash": "2e35b1f3b2666f0e402b0693dd7493a583002c98361385482aa9f27d8f0f5c89"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}